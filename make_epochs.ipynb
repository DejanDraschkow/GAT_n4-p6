{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import mne\n",
    "from autoreject import LocalAutoRejectCV\n",
    "mne.set_log_level('ERROR')\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "\n",
    "loc_files = \"/.../n4-p6/\" # location of previously saved raws, icas, and autoreject files\n",
    "template = \"{name}-{inst}.fif\" # template to load and save files\n",
    "\n",
    "df = pd.read_csv('/.../lookUp.csv') # load csv with sentence info\n",
    "event_ids = {\"con/hc\":201, \"con/lc\":200, \"inc/lc\":210, \"inc/hc\":211} # event ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bads = [\"VLA21\", # not a native speaker of German \n",
    "        \"GMA05\",\"RAC22\",\"PBT16\",\"HHA01\",\"BSN17\"] # rejected participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = list({fname.split(\"-\")[0]  # the names of all datasets\n",
    "              for fname in listdir(loc_files) \n",
    "              if \"raw\" in fname and fname.split(\"-\")[0] not in bads # excludes rejected participants\n",
    "              if \"files\" not in fname\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_events(events, srate=100):    \n",
    "    '''Look for critical word onsets for sentence items\n",
    "     calculates word onset relative to trigger event and \n",
    "     creates a new events file that indexes word onsets\n",
    "    events: events file from the raw data and with triggers and time points'''\n",
    "    \n",
    "    for cond, trg in event_ids.items():\n",
    "        inds = np.where(events[:, 2] == trg)[0]  # find all events belonging to this condition\n",
    "        congruency, cloze = cond.split(\"/\")\n",
    "        for ind in inds:\n",
    "            item = events[ind - 1, 2]  # check item trigger (precedes the condition trigger)\n",
    "            s = \"label == '{}_{}_{}'\".format(congruency, cloze, item - 1)\n",
    "            t = int((df.query(s)[\"onset\"].values[0])*srate)  # add word onset delay\n",
    "            \n",
    "            # 255 ms delay between trigger and audio onset which is taken into account\n",
    "            events[ind, 0] += t+(0.255*srate) \n",
    "    return events\n",
    "    \n",
    "def get_epoch(name, times=dict(tmin=-.3, tmax=1.3), highpass=0.1, lowpass=30):\n",
    "    '''Creates the epoched data for participant from -.3 to 1.3 relative to onset\n",
    "    name: participant\n",
    "    times: epoch time window tmin for start and tmax for end\n",
    "    '''\n",
    "    params = dict(name=name, inst='raw') # parmeters to insert into template\n",
    "    with mne.io.read_raw_fif(loc_files+template.format(**params)) as raw:\n",
    "        events = mne.find_events(raw, min_duration=0, shortest_event=0)\n",
    "        events = fix_events(events)         \n",
    "        raw.load_data()\n",
    "        raw.filter(highpass, lowpass, n_jobs=12, phase=\"zero\", filter_length='auto',\n",
    "                   l_trans_bandwidth='auto', h_trans_bandwidth='auto')\n",
    "        picks = mne.pick_types(raw.info, eeg=True, stim=False)        \n",
    "        return mne.Epochs(raw, events, event_ids, preload=True,\n",
    "                          baseline=(None, 0),\n",
    "                          picks=picks, tmin=-.3,tmax=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load previously calculted autoreject and repair\n",
    "import pickle\n",
    "with open(loc_files+'ar.pckl', 'rb') as f: \n",
    "        autorejs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ICAs\n",
    "icas=dict()\n",
    "for name in names:\n",
    "    params = dict(name=name, inst='ica')\n",
    "    icas[name] = mne.preprocessing.ica.read_ica(loc_files+'{name}-{inst}.fif'.format(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_epochs(epochs, name, on_ica=False):\n",
    "    '''Function to apply the autorejections and ICAs to the epoched data\n",
    "    here, we apply both autoreject and the ICA. \n",
    "    '''    \n",
    "    ica = icas[name]\n",
    "    # apply previously calculated ICA to exclude eye ICs\n",
    "    epochs = ica.apply(epochs.load_data(), exclude=ica.labels_[\"eog\"])\n",
    "    \n",
    "    # reject trials based on previously fitted thresholds and interpolate bad channels\n",
    "    epochs = autorejs[name].transform(epochs)\n",
    "    if on_ica:\n",
    "        epochs = ica.get_sources(epochs).drop_channels(\n",
    "            ['ICA0{:02d}'.format(ii) for ii in ica.labels_[\"eog\"]])\n",
    "        #epochs.drop_channels([\"SO1\", \"SO2\", \"FP1\", \"FP2\"])\n",
    "        mapping = {name:'eeg' for name in epochs.ch_names}\n",
    "        epochs.set_channel_types(mapping);\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saving_epochs(name, on_ica=False,highpass=.1, lowpass=30):\n",
    "    ''' function to load epoch, apply ICA correction and autorejection & repair\n",
    "     saves epoch to specified folder'''    \n",
    "    epochs = get_epoch(name,times={'tmin': -0.3, 'tmax': 1.3}, highpass=highpass, lowpass=lowpass)\n",
    "    epochs = clean_epochs(epochs, name, on_ica=on_ica)\n",
    "    params = dict(name=name, OIs=(\"on_ica-\" if on_ica else \"\"),highpass=highpass, lowpass=lowpass, inst=\"epo\")\n",
    "    #epochs.save(loc_files+\"other-filters/{name}-{OIs}{highpass}-{lowpass}-{inst}.fif\".format(**params))\n",
    "    epochs.save(loc_files+\"{name}-{OIs}{inst}.fif\".format(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for name in names:\n",
    "    [saving_epochs(name, on_ica=item, highpass=0.1, lowpass=30) for item in [True, False]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
